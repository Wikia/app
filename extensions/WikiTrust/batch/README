PREREQUISITES:
=============

WikiTrust is written in Ocaml.  You will need Ocaml-8.10.0 or later. 

You also need to get, and install, OcamlLdaLibs. 
These libraries are distributed from http://trust.cse.ucsc.edu/ 


BUILDING THE CODE:
=================

First, you need to make and install OcamlLdaLibs. 
Look at the README file in that directory for instructions on how to
proceed. 

To build all the WikiTrust executable, do: 

make all 

-or- 

make allopt

The [opt] versions of the command use the Ocaml optimizing compiler,
which is necessary if you plan to run the code on any extensive
collection of data. 
The non-opt versions genrates the code that is suitable to the Ocaml
debugger ocamldebug. 


GETTING HELP:
============

To have more information on any command, use: 

<command> --help


SPLITTING THE WIKI DUMP:
=======================

For reasons of practicality, we split a Wikipedia dump into smaller
dumps before processing.  

gunzip -c ~/work/wiki-data/wikis/enwiki-20070206.xml.gz | ./splitwiki -n 100 -p ~/work/wiki-data/split_wikis/enwiki-20070206_

This splits a wiki in chunks of 100 pages, compressing with gzip the
results (compression with other programs, such as 7z, turned out to be
too slow). 
Obviously, use the appropriate decompress program above; the intent is
to decompress to standard output. 

EXTRACTING A SUBSET OF PAGES FROM THE WIKI DUMP:
=======================

We can also extract a subset of pages based on page id from a
Wikipedia dump before processing.  

gunzip -c ~/work/wiki-data/wikis/enwiki-20070206.xml.gz | extract_wiki_subset -t good-ids -o enwiki-20070206-subset

or (for a 7za style archive)

7za e -so ../../wiki-dumps-ian/kuwiki-20080801-pages-meta-history.xml.7z | extract_wiki_subset -t good-ids -o kuwiki-20080801-subset

These both extract the pages who's page id is found in the file
good-ids, compressing with gzip the results (compression with other
programs, such as 7z, turned out to be too slow). 

Obviously, use the appropriate decompress program above; the intent is
to decompress to standard output. 

The page id's file expects 1 page id per line.

For example, to extract the page numbers 444 and 888 from a dump, the
file would look like

444
888

The ordering of page ids does not matter.

COMPUTING THE REDUCED STATISTCS:
===============================

First, we need to compute the "reduced statistics" for a wiki file. 
This is a concise summary of the text distances and new text added in
each revision. 
The command you need is: 

./evalwiki -d ~/work/wiki-data/stats/ -compute_stats ~/work/wiki-data/split_wikis/enwiki-20070206_00066.xml.gz

The above command puts the result in the stats/ directory.  The result
is a file called enwiki-20070206_00066.out . The file contains
statistics sorted in page order, not in cronological order. 

For small files, you can sort them in chronological order as follows: 

sort -n -k 2,2 -o <sorted_file> <file>

For bigger files, you can use a bucket merge/sort procedure we wrote in Ocaml
called combinestats:

./combinestats -outfile <single-stats-file> <dir where stats files are stored>

Note that combinestats generates a directory called ./SORTEDSTATS to store 
the stat buckets. 


COMPUTING THE REPUTATION FROM A STATISTICS FILE: 
===============================================

To compute a chronological history of user reputations, you can use
the following command: 

./generate_reputation -local_global_algo -u ~/work/wiki-data/temp/reph.txt ~/work/wiki-data/temp/enwiki-20070206_00066.stats 

Don't worry about the output; it is some testing information we use 
to judge the performance of the reputation.  Disregard it. 
The important thing is that the command generates a user reputation 
history file in ~/work/wiki-data/temp/reph.txt.  This is the file you
need to color the revision according to text trust. 

Note: The code will give a warning if the statistics file is not
properly sorted. 

Note: you cannot compute author reputations from only part of the
statistics file, like the ones corresponding to a 100-article block
only.  This would be equivalent to computing reputation histories of
authors, as if only those articles existed in the wiki! 
You need to do this for all articles at once. 

With this, you can produce a history of user reputations over the
Wikipedia; I will assume that this will be called 

rep-histories/enwiki-20070206-rel1.0-users

If users are anonymous, the reputation of their domains can be computed 
using the following additional commands to generate_reputation -

-domains

The parameter -domains will ensure that anonymous user domains is included
in the computation of reputations. In such cases, a unique user id is
generated from their domain ip addresses.

-ip_nbytes <n>

The parameter -ip_nbytes can be used to specify the number of bytes [1, 4]
that should be used to generate the unique user ids. For instance, the 
address 172.162.56.61, with -ip_nbytes set to 1, will generate as user id
-172, whereas the same address with -ip_nbytes set to 3, will generate
a user id -11313720.

The generate_reputation program has many command-line options, some of
them referring to the algorithms to be used for reputation
computation; do ./generate_reputation -help for more information. 

COLORING A WIKI ACCORDING TO TRUST: 
==================================

Coloring a wiki takes as input a compressed .xml dump (compressed with
gzip, by default), and produces an uncompressed .xml file containing
the colored markup language.  The resulting file can be loaded into a
mediawiki database using mwdumper (see below for help). 

There are many coefficients that determine how to color a wiki file. 
The following is an example that uses many of the available options: 

./evalwiki -d ~/work/wiki-data/colored_wikis/ -color_local_trust -historyfile ~/work/wiki-data/rep-histories/enwiki-20070206-rel1.0-users -rep_lends_trust 0.4 -trust_read_all 0.2 -trust_read_part 0.2 -trust_radius 2.0 -trust_part_radius 4.0 -n_rev_to_color 50 ~/work/wiki-data/split_wikis/enwiki-20070206_00066.xml.gz

Note that one can use a regexp, i.e.,
~/work/wiki-data/split_wikis/enwiki-20070206_* to color many wikis at
once.

Some parameters are worth noticing: 

This tells that we want to produce trust coloring.  If we also want to
have provenance information, use -trust_and_origin as the flag.

  -color_local_trust 

These parameters describe the numerical method used for trust
coloring.  They have reasonable defaults.  Look at the technical
report on trust computation for more information. 

  -rep_lends_trust 0.4 
  -trust_read_all 0.2 
  -trust_read_part 0.2 
  -trust_radius 2.0 
  -trust_part_radius 4.0

This says that, for each article, we have to output only the most
recent 50 revisions:

  -n_rev_to_color 50

This is the file with the history of user reputations.  It has to
match the wiki being colored!

  -historyfile ~/work/wiki-data/rep-histories/enwiki-20070206-rel1.0-users


LOADING THE COLORED WIKI IN A DATABASE: 
======================================

This varies somewhat depending on the name and password of the local
database.

First, reset the databaseif you have already loaded the same
articles, or if you wish to empty it:

scripts/reset_wiki.sh

Then, load the wiki: 

/load_wiki.sh ~/work/wiki-data/colored_wikis/enwiki-20070206_00066.xml

And hope that it all went well. 

EXTRACTING AUTHOR CONTRIBUTIONS
===============================

This feature can be used to extract author contributions as the weighted
average of the longevity of edits, weighted by the quantity of such edits.
This can be done from the statistics extracted through evalwiki. The 
following command can be used for this purpose -

./generate_reputation [-a] -u_contrib <filename> [-u_contrib_order_asc]
 <statistics_filename>

The -u_contrib <filename> notifies generate_reputation that we would like
to extract author contributions, by default in descending order into the
file named <filename>. 

The parameter -a is optional, which when present will include anonymous
users in the analysis.

The parameter -u_contrib_order_asc is optional, which when present
changes the order in which authors are listed in the contributions file
from being descending to ascending.

The final parameter <statistics_filename> is the name of the file with
statistics extracted from the wiki pages. These statistics can be 
extracted using evalwiki as follows -

./evalwiki -d <dest_dirname> -compute_stats <wiki_filename>

Here, the -d is used to specify <dest_dirname> as the directory in which 
we would like to evalwiki to place the extracted statistics file.

The -compute_stats switch instructs evalwiki to extract statistics.

The <wiki_filename> is the name of the file that contains the wiki data.
This can be either the wiki data xml file or a gzipped version of it.

A file called contributions.txt resides in the same directory as this
README file and contains an example of the file extracted with author
names and contributions. The file was generated from a small sample
statistics file and includes anonymous authors.
